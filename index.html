<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Shalini Maiti ‚Äî Research CV</title>
    <meta name="description" content="PhD at Meta Superintelligence Labs & University College London ‚Äî 3D Computer Vision and Language Models">
    <link rel="stylesheet" href="style.css">
    <script defer src="app.js"></script>
  </head>
  <body>
    <header class="site-header">
      <div class="container header-inner">
        <div class="avatar">
          <img src="images/profile.jpg" alt="Profile photo of Shalini Maiti">
        </div>
        <div class="title">
          <h1>Shalini Maiti</h1>
          <p class="tagline">PhD at Meta Superintelligence Labs & University College London ‚Äî 3D Computer Vision and Language Models</p>
          <ul class="meta">
            <li>üìç London, United Kingdom</li>
            <li>‚úâÔ∏è <a href="mailto:shalini.maiti@gmail.com">shalini.maiti@gmail.com</a></li>
            <li>üêô <a href="https://github.com/shalini-maiti" target="_blank">GitHub</a></li>
            <li>üíº <a href="https://www.linkedin.com/in/shalini-maiti-a76a2b86/" target="_blank">LinkedIn</a></li>
          </ul>
        </div>
      </div>
    </header>

    <main class="container">
      <div class="tabs">
        <button class="tab active" data-tab="research">üî¨ Research</button>
        <button class="tab" data-tab="cv">üìö CV</button>
        <button class="tab" data-tab="projects">üíª Projects</button>
        <button class="tab" data-tab="travel">‚úàÔ∏è Travel</button>
        <button class="tab" data-tab="poetry">üìù Poetry</button>
        <button class="tab" data-tab="about">üëã About</button>
      </div>

      <section class="tab-panel active" id="research">
        <h2>Research Papers</h2>
        <div class="card">
          <h3>Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects</h3>
          <p><strong>Shalini Maiti</strong>; Lourdes Agapito; Filippos Kokkinos ‚Äî CVPR 2025</p>
          <strong>Abstract:</strong> Rapid advancements in text-to-3D generation require robust and scalable evaluation metrics that align closely with
          human judgment, a need unmet by current metrics such as
          PSNR and CLIP, which require ground-truth data or focus only on prompt fidelity. To address this, we introduce
          Gen3DEval, a novel evaluation framework that leverages
          vision large language models (vLLMs) specifically finetuned for 3D object quality assessment. Gen3DEval evaluates text fidelity, appearance, and surface quality by analyzing 3D surface normals, without requiring ground-truth
          comparisons, bridging the gap between automated metrics
          and user preferences. Compared to state-of-the-art taskagnostic models, Gen3DEval demonstrates superior performance in user-aligned evaluations, placing it as a comprehensive and accessible benchmark for future research on
          text-to-3D generation. The project page can be found <a href="https://shalini-maiti.github.io/gen3deval.github.io/" target="_blank">here</a>.
          <!-- <ul>
            <li>Finetuned a vLLM for holistic, automated evaluation of generated 3D assets.</li>
            <li>Introduced Gen3DEval-Bench + leaderboard.</li>
          </ul> -->
        </div>

        <div class="card">
          <h3>Unsupervised 2D‚Äì3D Lifting of Non-Rigid Objects Using Local Constraints</h3>
          <p><strong>Shalini Maiti</strong>; Lourdes Agapito; Benjamin Graham ‚Äî CVPR 4D Vision Workshop 2025</p>
          <strong>Abstract:</strong> For non-rigid objects, predicting the 3D shape from 2D keypoint observations is ill-posed due to occlusions, and the
          need to disentangle changes in viewpoint and changes in
          shape. This challenge has often been addressed by embedding low-rank constraints into specialized models. These
          models can be hard to train, as they depend on finding a
          canonical way of aligning observations, before they can
          learn detailed geometry. These constraints have limited the
          reconstruction quality. We show that generic, high capacity
          models, trained with an unsupervised loss, allow for more
          accurate predicted shapes. In particular, applying low-rank
          constraints to localized subsets of the full shape allows the
          high capacity to be suitably constrained. We reduce the
          state-of-the-art reconstruction error on the S-Up3D dataset
          by over 70%.
        </div>

        <div class="card">
          <h3>VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for LVLMs</h3>
          <p>Yufan Ren; Konstantinos Tertikas; <strong>Shalini Maiti</strong> ‚Äî arXiv 2025</p>
          <strong>Abstract:</strong> Large Vision-Language Models (LVLMs) struggle with puzzles, which require precise perception, 
          rule comprehension, and logical reasoning. Assessing and enhancing their performance in this domain is crucial, 
          as it reflects their ability to engage in structured reasoning ‚Äî an essential skill
          for real-world problem-solving. However, existing benchmarks primarily evaluate pre-trained models without additional training or 
          fine-tuning, often lack a dedicated focus on reasoning, and fail to establish a systematic evaluation framework. 
          To address these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning Puzzle Benchmark featuring 20 diverse puzzles. 
          VGRP-Bench spans multiple difficulty levels, and includes extensive experiments not only
          on existing chat LVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). 
          Our results reveal that even the state-of-the-art LVLMs struggle with these puzzles, 
          highlighting fundamental limitations in their puzzle-solving capabilities. 
          Most importantly, through systematic experiments, we identify and analyze key factors influencing
          LVLMs‚Äô puzzle-solving performance, including the number of clues, grid size, and rule complexity. 
          Furthermore, we explore two Supervised Fine-Tuning (SFT) strategies that can be used in post-training: 
          SFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT). While both
          methods significantly improve performance on trained puzzles, they exhibit limited generalization to unseen ones. 
          We will release VGRP-Bench to facilitate further research on
          LVLMs for complex, real-world problem-solving. <a href="https://yufan-ren.com/subpage/VGRPBench/" target="_blank">Project
          page</a>
        </div>

        <!-- <div class="card">
          <h3><a href="docs/souper_model_magg_arr_2026.pdf" target="_blank">Souper-Model (SoCE): How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</a></h3>
          <p><strong>Shalini Maiti*</strong>; Amar Budhiraja*; Bhavul Gauri; Gaurav Chaurasia; Anton Protopopov; Alexis Audran-Reiss; Michael Slater; Despoina Magka; Tatiana Shavrina; Roberta Raileanu; Yoram Bachrach ‚Äî ARR under review 2026</p>
        </div> -->

        <h2>Theses</h2>
        <ul>
          <li><a href="docs/masters_thesis_shalini_maiti.pdf" target="_blank">Master‚Äôs Thesis ‚Äî Domain Randomization for Hand Pose Estimation (TU Graz, 2020)</a></li>
          <li><a href="docs/btp_report.pdf" target="_blank">Bachelor‚Äôs Thesis ‚Äî Conversational AI: Understanding Conversations & Bots (DA-IICT, 2014)</a></li>
        </ul>
      </section>

      <section class="tab-panel" id="cv">
        <h2>Curriculum Vitae</h2>
        <a href="docs/cv_shalini_maiti.pdf" class="pill" target="_blank">üìÑ Download Full CV (PDF)</a>
        <div class="grid">
          <div>
            <h3>Education</h3>
            <ul>
              <li>PhD, University College London: 2022 ‚Äì Present</li>
              <li>MSc, TU Graz: 2018 ‚Äì 2021</li>
              <li>B.Tech, DA-IICT: 2010 ‚Äì 2014</li>
            </ul>
          </div>
          <div>
            <h3>Experience</h3>
            <ul>
              <li>Meta Superintelligence Labs, Research Scientist: 2022 - Present </li>
              <li>Amazon Prime Air, SDE: 2021 - 2022</li>
              <li>Insitute of Computer Vision and Graphics (TU Graz), Student Project Assistant: 2019 - 2021</li>
              <li>Cybrilla, Full Stack Developer: 2016 - 2018 </li>
              <li>Candyman Entertainment, Co-Founder: 2015 - 2016 </li>
              <li>Keepworks, Full Stack Developer: 2014 - 2015 </li>
            </ul>
          </div>
        </div>
      </section>

      <section class="tab-panel" id="projects">
        <h2>GitHub Projects</h2>
        <p>Visit my GitHub for open-source work ‚Üí <a href="https://github.com/shalini-maiti">github.com/shalini-maiti</a></p>
      </section>

      <section class="tab-panel" id="travel">
        <h2>Travel Log</h2>
        <ul>
          <!-- <li>üçµ Kyoto, Japan (2024) ‚Äî Tea, temples, trains.</li>
          <li>üåå Reykjav√≠k, Iceland (2023) ‚Äî Aurora and hot springs.</li>
          <li>üçÆ Lisbon, Portugal (2023) ‚Äî Past√©is + streetcars.</li> -->
        </ul>
      </section>

      <section class="tab-panel" id="poetry">
        <h2>Pocket Poems</h2>
          <!-- <pre>code hums low
  keys like rain on tin
  thoughts knit constellations
  while the kettle decides
  to be a comet</pre>

          <pre>two shirts
  one book
  three sunsets
  a handful of new hellos
  all of it fits</pre> -->
      </section>

      <section class="tab-panel" id="about">
        <h2>About</h2>
        <p>I‚Äôm a researcher exploring intersections of 3D computer vision and large language models. Currently pursuing my PhD at University College London and Meta AI.</p>
      </section>
    </main>

    <footer class="site-footer">
      <p>Built with ‚ù§Ô∏è ¬∑ ¬© 2025 Shalini Maiti</p>
    </footer>
  </body>
</html>
